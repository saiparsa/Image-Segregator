{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Face.py","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"NuYGdRkXFZOX","colab_type":"code","colab":{}},"source":["from imutils import paths\n","import numpy as np\n","import argparse\n","import imutils\n","import pickle\n","import cv2\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.svm import SVC\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3NXGCrdqJ3Ao","colab_type":"code","outputId":"0de0c130-24c0-4b51-b298-c07285cac17a","executionInfo":{"status":"ok","timestamp":1575316720347,"user_tz":300,"elapsed":29557,"user":{"displayName":"aml mla","photoUrl":"","userId":"12337879236727089287"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P0HCaAxJqbi_","colab_type":"code","outputId":"10a10b08-d850-433d-94e2-250ec5a7cc74","executionInfo":{"status":"ok","timestamp":1575316735732,"user_tz":300,"elapsed":1116,"user":{"displayName":"aml mla","photoUrl":"","userId":"12337879236727089287"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd ./drive/My Drive/AML_Project/Face/face_detection_model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/AML_Project/Face/face_detection_model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EckZDpjXpy1z","colab_type":"code","outputId":"b983fb52-52b9-4d3d-cc77-9877c41b0e87","executionInfo":{"status":"ok","timestamp":1575316759741,"user_tz":300,"elapsed":24075,"user":{"displayName":"aml mla","photoUrl":"","userId":"12337879236727089287"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["detector = cv2.dnn.readNetFromCaffe('deploy.prototxt','res10_300x300_ssd_iter_140000.caffemodel')#OpenCV’s Caffe-based deep learning face detector used to localize faces in the images.\n","%cd ..\n","embedder = cv2.dnn.readNetFromTorch('openface_nn4.small2.v1.t7')#A Torch deep learning model which produces the 128-D facial embeddings."],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/AML_Project/Face\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bc1ZLgkDryg_","colab_type":"code","colab":{}},"source":["\n","knownNames=[]\n","knownEmbeddings=[]\n","\n","\n","imagePaths = list(paths.list_images('/content/drive/My Drive/AML_Project/Face/dataset/'))\n","\n","\n","for path in imagePaths:\n","    name = path.split(os.path.sep)[-2]\n","    image = cv2.imread(path)\n","    #image=image/255\n","    image = imutils.resize(image, width=600)\n","    (h, w) = image.shape[:2]\n","    imageBlob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300),(104.0, 177.0, 123.0), swapRB=False, crop=False)\n","    detector.setInput(imageBlob)\n","    detections = detector.forward()\n","    #print(len(detections))\n","    if len(detections) > 0:\n","        i = np.argmax(detections[0, 0, :, 2])\n","        confidence = detections[0, 0, i, 2]\n","    \n","    if confidence > 0.5:\n","        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","        (startX, startY, endX, endY) = box.astype(\"int\")\n","        face = image[startY:endY, startX:endX]\n","        (fH, fW) = face.shape[:2]\n","        \n","        if fW < 20 or fH < 20:\n","            continue\n","        \n","        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,(96, 96), (0, 0, 0), swapRB=True, crop=False)\n","        embedder.setInput(faceBlob)\n","        vec = embedder.forward()\n","\n","        \n","        knownNames.append(name)\n","        knownEmbeddings.append(vec.flatten())\n","data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNNbRNyF08M9","colab_type":"code","outputId":"f1be550b-cc33-4846-b99c-62068583d838","executionInfo":{"status":"ok","timestamp":1575317360653,"user_tz":300,"elapsed":886,"user":{"displayName":"aml mla","photoUrl":"","userId":"12337879236727089287"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["le = LabelEncoder()\n","labels = le.fit_transform(data['names'])\n","recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n","recognizer.fit(data['embeddings'], labels)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n","    kernel='linear', max_iter=-1, probability=True, random_state=None,\n","    shrinking=True, tol=0.001, verbose=False)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"oQ6IODhzGAr-","colab_type":"code","outputId":"6f4d92ad-d5ca-42f1-ac3f-9ebaca47c4a1","executionInfo":{"status":"ok","timestamp":1575318966163,"user_tz":300,"elapsed":16361,"user":{"displayName":"aml mla","photoUrl":"","userId":"12337879236727089287"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Vq_OpdRh_AAedhkXMo-MaofnzvFwW4Ca"}},"source":["from google.colab.patches import cv2_imshow\n","count=0\n","uk=0\n","priorityc=0\n","otherc=0 \n","    \n","testPaths = list(paths.list_images('/content/drive/My Drive/AML_Project/Test1/Test1_Classified'))\n","\n","for path in testPaths:\n","    names=[]\n","    count1=str(count)\n","    image = cv2.imread(path)\n","    image = imutils.resize(image, width=600)\n","    (h, w) = image.shape[:2]\n","\n","    # construct a blob from the image\n","    imageBlob = cv2.dnn.blobFromImage(\n","        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n","        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n","\n","    # apply OpenCV's deep learning-based face detector to localize\n","    # faces in the input image\n","    detector.setInput(imageBlob)\n","    detections = detector.forward()\n","\n","    # loop over the detections\n","    for i in range(0, detections.shape[2]):\n","        # extract the confidence (i.e., probability) associated with the\n","        # prediction\n","        confidence = detections[0, 0, i, 2]\n","        \n","        # filter out weak detections\n","        if confidence > 0.50 :\n","            # compute the (x, y)-coordinates of the bounding box for the\n","            # face\n","            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","\n","            # extract the face ROI\n","            face = image[startY:endY, startX:endX]\n","            (fH, fW) = face.shape[:2]\n","\n","            # ensure the face width and height are sufficiently large\n","            if fW < 20 or fH < 20:\n","                continue\n","\n","            # construct a blob for the face ROI, then pass the blob\n","            # through our face embedding model to obtain the 128-d\n","            # quantification of the face\n","            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n","                (0, 0, 0), swapRB=True, crop=False)\n","            embedder.setInput(faceBlob)\n","            vec = embedder.forward()\n","\n","            # perform classification to recognize the face\n","            preds = recognizer.predict_proba(vec)[0]\n","            j = np.argmax(preds)\n","            proba = preds[j]\n","            name = le.classes_[j]\n","        \n","            names.append(name)\n","            \n","            \n","            \n","    for i in names:\n","        if i=='unknown':\n","            uk+=1\n","    \n","\n","    if uk== len(names) :\n","        cv2.imwrite('others/'+str(otherc)+'.jpg', image)\n","        otherc+=1\n","        print('Others')\n","        cv2_imshow(image)\n","    else:   \n","        cv2.imwrite('Priority images/'+str(priorityc)+'.jpg', image)\n","        #print('Boo')\n","        priorityc+=1\n","        print('Priority')\n","        cv2_imshow(image)\n","    uk=0 \n","    names=[]\n","    count+=1\n","print(\"No.of images in Priority\",priorityc)\n","print(\"No.of iamges in others\",otherc)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"Ao9rjO5znmOv","colab_type":"code","outputId":"f084d03a-e880-4faf-d82f-34edc2f07d34","executionInfo":{"status":"ok","timestamp":1575319517995,"user_tz":300,"elapsed":852,"user":{"displayName":"aml mla","photoUrl":"","userId":"12337879236727089287"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["Priority_images_in_test1=6 # The number of priority images in the test set \n","Classified_as_priority= priorityc #no.of images the model identified as priority\n","\n","Accuracy= Priority_images_in_test1/Classified_as_priority\n","print (Accuracy*100,'percent')# Though all the priority images are correctly classified there are 2 images which are wrongly classified"],"execution_count":0,"outputs":[{"output_type":"stream","text":["75.0 percent\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ueeWggEcOSwd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}